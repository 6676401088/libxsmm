<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>LIBXSMM by hfp</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <script src="javascripts/scale.fix.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1 class="header">LIBXSMM</h1>
        <p class="header">Library for small matrix-matrix multiplications targeting Intel Architecture (x86).</p>

        <ul>
          <li class="download"><a class="buttons" href="https://github.com/hfp/libxsmm/zipball/master">Download ZIP</a></li>
          <li class="download"><a class="buttons" href="https://github.com/hfp/libxsmm/tarball/master">Download TAR</a></li>
          <li><a class="buttons github" href="https://github.com/hfp/libxsmm">View On GitHub</a></li>
        </ul>

        <p class="header">This project is maintained by <a class="header name" href="https://github.com/hfp">hfp</a></p>


      </header>
      <section>
        <h1>
<a id="libxsmm" class="anchor" href="#libxsmm" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>LIBXSMM</h1>

<p>LIBXSMM is a library for small dense and small sparse matrix-matrix multiplications targeting Intel Architecture (x86). The library is generating code for the following instruction set extensions: Intel SSE, Intel AVX, Intel AVX2, IMCI (KNCni) for Intel Xeon Phi coprocessors ("KNC"), and Intel AVX-512 as found in the <a href="https://software.intel.com/en-us/articles/what-disclosures-has-intel-made-about-knights-landing">Intel Xeon Phi processor family ("KNL")</a> and future Intel Xeon processors. Historically the library was solely targeting the Intel Many Integrated Core Architecture "MIC") using intrinsic functions, meanwhile optimized assembly code is targeting all aforementioned instruction set extensions (static code generation), and Just-In-Time (JIT) code generation is targeting Intel AVX and beyond. [<a href="https://github.com/hfp/libxsmm/raw/master/documentation/libxsmm.pdf">pdf</a>] [<a href="https://github.com/hfp/libxsmm/archive/1.1.1.zip">src</a>] <a href="https://github.com/hfp/libxsmm/archive/master.zip"><img src="https://travis-ci.org/hfp/libxsmm.svg?branch=master" alt="cistatus" title="Master branch build status"></a> <a href="https://codecov.io/github/hfp/libxsmm?branch=master"><img src="https://codecov.io/github/hfp/libxsmm/coverage.svg?branch=master" alt="coverage"></a></p>

<p><strong>What is the background of the name "LIBXSMM"?</strong> The "MM" stands for Matrix Multiplication, and the "S" clarifies the working domain i.e., Small Matrix Multiplication. The latter also means the name is neither a variation of "MXM" nor an eXtreme Small Matrix Multiplication but rather about Intel Architecture (x86) - and no, the library is 64-bit (only). The spelling of the name might follow the syllables of libx\/smm, libx'smm, or libx-smm.</p>

<p><strong>What is a small matrix-matrix multiplication?</strong> When characterizing the problem size using the M, N, and K parameters, a problem size suitable for LIBXSMM falls approximately within (M N K)<sup>1/3</sup> &lt;= 80 (which illustrates that non-square matrices or even "tall and skinny" shapes are covered as well). However the code generator only generates code up to the specified <a href="#auto-dispatch">threshold</a>. Raising the threshold may not only generate excessive amounts of code (due to unrolling in M and K dimension), but also miss to implement a tiling scheme to effectively utilize the L2 cache. For problem sizes above the configurable threshold, LIBXSMM is falling back to BLAS.</p>

<p><strong>How to determine whether an application can benefit from using LIBXSMM or not?</strong> Given the application uses BLAS to carry out matrix multiplications, one may link against Intel MKL 11.2 (or higher), set the environment variable MKL_VERBOSE=1, and run the application using a representative workload (env MKL_VERBOSE=1 ./workload &gt; verbose.txt). The collected output is the starting point for evaluating the problem sizes as imposed by the workload (grep -a "MKL_VERBOSE DGEMM" verbose.txt | cut -d, -f3-5).</p>

<h2>
<a id="interface" class="anchor" href="#interface" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interface</h2>

<p>The interface of the library is <em>generated</em> according to the <a href="#build-instructions">Build Instructions</a>, and is therefore <strong>not</strong> stored in the code repository. Instead, one may have a look at the code generation template files for <a href="https://github.com/hfp/libxsmm/blob/master/src/libxsmm.template.h">C/C++</a> and <a href="https://github.com/hfp/libxsmm/blob/master/src/libxsmm.template.f">FORTRAN</a>.</p>

<p>In order to initialize the dispatch-table or other internal resources, one may call an explicit initialization routine in order to avoid lazy initialization overhead when calling LIBXSMM for the first time. The library deallocates internal resources automatically, but also provides a companion to the aforementioned initialization (finalize).</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** Initialize the library; pay for setup cost at a specific point. */</span>
<span class="pl-k">void</span> <span class="pl-en">libxsmm_init</span>(<span class="pl-k">void</span>);
<span class="pl-c">/** Uninitialize the library and free internal memory (optional). */</span>
<span class="pl-k">void</span> <span class="pl-en">libxsmm_finalize</span>(<span class="pl-k">void</span>);</pre></div>

<p>To perform the dense matrix-matrix multiplication <em>C<sub>m x n</sub> = alpha · A<sub>m x k</sub> · B<sub>k x n</sub> + beta · C<sub>m x n</sub></em>, the full-blown GEMM interface can be treated with "default arguments" (which is deviating from LAPACK/BLAS standard however without compromising the binary compatibility).</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** Call automatically dispatched dense matrix multiplication (single/double-precision, C code). */</span>
libxsmm_?gemm(<span class="pl-c1">NULL</span><span class="pl-c">/*transa*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*transb*/</span>, &amp;m<span class="pl-c">/*required*/</span>, &amp;n<span class="pl-c">/*required*/</span>, &amp;k<span class="pl-c">/*required*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*alpha*/</span>, a<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*lda*/</span>, b<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldb*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*beta*/</span>, c<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldc*/</span>);
<span class="pl-c">/** Call automatically dispatched dense matrix multiplication (C++ code). */</span>
<span class="pl-en">libxsmm_gemm</span>(<span class="pl-c1">NULL</span><span class="pl-c">/*transa*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*transb*/</span>, m<span class="pl-c">/*required*/</span>, n<span class="pl-c">/*required*/</span>, k<span class="pl-c">/*required*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*alpha*/</span>, a<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*lda*/</span>, b<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldb*/</span>,
  <span class="pl-c1">NULL</span><span class="pl-c">/*beta*/</span>, c<span class="pl-c">/*required*/</span>, <span class="pl-c1">NULL</span><span class="pl-c">/*ldc*/</span>);</pre></div>

<p>For the C interface (with type prefix 's' or 'd'), all arguments and in particular m, n, and k are passed by pointer. This is needed for binary compatibility with the original GEMM/BLAS interface. The C++ interface is also supplying overloaded versions where m, n, and k are allowed to be passed by-value (making it clearer that m, n, and k are non-optional arguments).</p>

<p>The Fortran interface supports optional arguments (without affecting the binary compatibility with the original LAPACK/BLAS interface) by allowing to omit arguments (where the C/C++ interface is allowing NULL to be passed). For convenience, a similar BLAS-based dense matrix multiplication (libxsmm_blas_gemm instead of libxsmm_gemm) is provided for all supported languages which is simply re-exposing the underlying GEMM/BLAS implementation. However, the re-exposed functions perform argument twiddling to account for ROW_MAJOR storage order (if enabled). The BLAS-based GEMM might be useful for validation/benchmark purposes, and more important as a fallback implementation when building an application-specific dispatch mechanism.</p>

<div class="highlight highlight-source-fortran-modern"><pre><span class="pl-c">! Call automatically dispatched dense matrix multiplication (single/double-precision).</span>
<span class="pl-k">CALL</span> libxsmm_?gemm(m<span class="pl-k">=</span>m, n<span class="pl-k">=</span>n, k<span class="pl-k">=</span>k, a<span class="pl-k">=</span>a, b<span class="pl-k">=</span>b, c<span class="pl-k">=</span>c)
<span class="pl-c">! Call automatically dispatched dense matrix multiplication (generic interface).</span>
<span class="pl-k">CALL</span> libxsmm_gemm(m<span class="pl-k">=</span>m, n<span class="pl-k">=</span>n, k<span class="pl-k">=</span>k, a<span class="pl-k">=</span>a, b<span class="pl-k">=</span>b, c<span class="pl-k">=</span>c)</pre></div>

<p>Successively calling a particular kernel (i.e., multiple times) allows for amortizing the cost of the code dispatch. Moreover in order to customize the dispatch mechanism, one can rely on the following interface.</p>

<div class="highlight highlight-source-c"><pre><span class="pl-c">/** If non-zero function pointer is returned, call (*function_ptr)(a, b, c). */</span>
libxsmm_smmfunction <span class="pl-en">libxsmm_smmdispatch</span>(<span class="pl-k">int</span> m, <span class="pl-k">int</span> n, <span class="pl-k">int</span> k,
  <span class="pl-k">const</span> <span class="pl-k">int</span>* lda, <span class="pl-k">const</span> <span class="pl-k">int</span>* ldb, <span class="pl-k">const</span> <span class="pl-k">int</span>* ldc,
  <span class="pl-k">const</span> <span class="pl-k">float</span>* alpha, <span class="pl-k">const</span> <span class="pl-k">float</span>* beta,
  <span class="pl-k">const</span> <span class="pl-k">int</span>* flags, <span class="pl-k">const</span> <span class="pl-k">int</span>* prefetch);
<span class="pl-c">/** If non-zero function pointer is returned, call (*function_ptr)(a, b, c). */</span>
libxsmm_dmmfunction <span class="pl-en">libxsmm_dmmdispatch</span>(<span class="pl-k">int</span> m, <span class="pl-k">int</span> n, <span class="pl-k">int</span> k,
  <span class="pl-k">const</span> <span class="pl-k">int</span>* lda, <span class="pl-k">const</span> <span class="pl-k">int</span>* ldb, <span class="pl-k">const</span> <span class="pl-k">int</span>* ldc,
  <span class="pl-k">const</span> <span class="pl-k">double</span>* alpha, <span class="pl-k">const</span> <span class="pl-k">double</span>* beta,
  <span class="pl-k">const</span> <span class="pl-k">int</span>* flags, <span class="pl-k">const</span> <span class="pl-k">int</span>* prefetch);</pre></div>

<p>A variety of overloaded function signatures is provided allowing to omit arguments not deviating from the configured defaults. Moreover, in C++ a type 'libxsmm_mmfunction&lt;<em>type</em>&gt;' can be used to instantiate a functor rather than making a distinction for the numeric type in 'libxsmm_?mmdispatch'. Similarly in Fortran, when calling the generic interface (libxsmm_mmdispatch) the given LIBXSMM_?MMFUNCTION is dispatched such that libxsmm_call can be used to actually perform the function call using the PROCEDURE POINTER wrapped by LIBXSMM_?MMFUNCTION. Beside of dispatching code, one can also call a specific kernel (e.g., 'libxsmm_dmm_4_4_4') using the prototype functions included for statically generated kernels.</p>

<h2>
<a id="build-instructions" class="anchor" href="#build-instructions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Build Instructions</h2>

<p>The build system relies on GNU Make (typically associated with the <code>make</code> command, but e.g. FreeBSD is calling it <code>gmake</code>). The build can be customized by using key-value pairs. Key-value pairs can be supplied in two ways: (1)\ after the "make" command, or (2)\ prior to the "make" command (<code>env</code>) which is effectively the same as exporting the key-value pair as an environment variable (<code>export</code>, or <code>setenv</code>). Of course both methods can be mixed, however the second method requires to supply the <code>-e</code> flag i.e., <code>env CC=gcc make -e</code> since the key-value pairs are not considered otherwise.</p>

<p>To generate the interface of the library inside of the 'include' directory and to build the static library (by default, STATIC=1 is activated), simply run the following command:</p>

<pre><code>make
</code></pre>

<p>By default, only the non-coprocessor targets are built (OFFLOAD=0 and MIC=0). In general, the subfolders of the 'lib' directory are separating the build targets where the 'mic' folder is containing the native library (MIC=1) targeting the Intel Xeon Phi coprocessor ("KNC"), and the 'intel64' folder is storing either the hybrid archive made of CPU and coprocessor code (OFFLOAD=1), or an archive which is only containing the CPU code. By default, an OFFLOAD=1 implies MIC=1.</p>

<p>To remove intermediate files, or to remove all generated files and folders (including the interface and the library archives), run one of the following commands:</p>

<pre><code>make clean
make realclean
</code></pre>

<p>The library can be configured to accept row-major or column-major (default) order matrices. The row-major storage scheme is accomplished by setting ROW_MAJOR=1 (0 for column-major, and row-major otherwise):</p>

<pre><code>make ROW_MAJOR=1
</code></pre>

<p>By default, LIBXSMM uses the <a href="#jit-backend">JIT backend</a> which is automatically building optimized code. However, one can also statically specialize for particular matrix sizes (M, N, and K values):</p>

<pre><code>make M="2 4" N="1" K="$(echo $(seq 2 5))"
</code></pre>

<p>The above example is generating the following set of (M,N,K) triplets:</p>

<pre><code>(2,1,2), (2,1,3), (2,1,4), (2,1,5),
(4,1,2), (4,1,3), (4,1,4), (4,1,5)
</code></pre>

<p>The index sets are in a loop-nest relationship (M(N(K))) when generating the indices. Moreover, an empty index set resolves to the next non-empty outer index set of the loop nest (including to wrap around from the M to K set). An empty index set is not participating anymore in the loop-nest relationship. Here is an example of generating multiplication routines which are "squares" with respect to M and N (N inherits the current value of the "M loop"):</p>

<pre><code>make M="$(echo $(seq 2 5))" K="$(echo $(seq 2 5))"
</code></pre>

<p>An even more flexible specialization is possible by using the MNK variable when building the library. It takes a list of indexes which are eventually grouped (using commas):</p>

<pre><code>make MNK="2 3, 23"
</code></pre>

<p>Each group of the above indexes is combined into all possible triplets generating the following set of (M,N,K) values:</p>

<pre><code>(2,2,2), (2,2,3), (2,3,2), (2,3,3),
(3,2,2), (3,2,3), (3,3,2), (3,3,3), (23,23,23)
</code></pre>

<p>Of course, both mechanisms (M/N/K and MNK based) can be combined using the same command line (make). Static optimization and JIT can also be combined (no need to turn off the JIT backend).</p>

<p>Testing the generated cases can be accomplished by capturing the console output of the <a href="https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k.cpp">cp2k</a> code sample:</p>

<pre><code>make MNK="2 3, 23" test
</code></pre>

<p>The recorded output file can be further evaluated (see also <a href="https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-test.sh">cp2k-test.sh</a>). For example:</p>

<pre><code>grep "diff" samples/cp2k/cp2k-perf.txt | grep -v "diff=0.000"
</code></pre>

<p><strong>NOTE</strong>: by default, a combination of a C/C++ and a Fortran compiler is needed (some sample code is written in C++). Beside of specifying the compilers (<code>make CXX=g++ CC=gcc FC=gfortran</code> and maybe <code>AR=ar</code>), the need for a Fortran compiler can be relaxed (<code>make FC=</code>). The latter affects the availability of the MODule file and the corresponding 'libxsmmf' library (the interface 'libxsmm.f' is still generated). Fortran code can make use of LIBXSMM in three different ways:</p>

<ul>
<li>By relying on the module file, and by linking against 'libxsmmf' and 'libxsmm' (this order),</li>
<li>By including the interface 'libxsmm.f', and by linking solely against 'libxsmm', or by</li>
<li>Optionally declaring 'libxsmm_?gemm' (like BLAS), and by linking against 'libxsmm'.</li>
</ul>

<p>At the expense of a limited functionality (libxsmm_?gemm), the latter method also works with Fortran 77 (otherwise the Fortran 2003 standard is necessary).</p>

<h2>
<a id="installation" class="anchor" href="#installation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

<p>Installing LIBXSMM makes possibly the most sense when combining the <a href="#jit-backend">JIT backend</a> (enabled by default) with a collection of statically generated SSE kernels (by specifying M, N, K, or MNK). If the JIT backend is not disabled, statically generated kernels are only registered for dispatch if the CPUID flags at runtime are not supporting a more specific instruction set extension (code path). Since the JIT backend does not support or generate SSE code by itself, the library is compiled by selecting SSE code generation if not specified otherwise (AVX=1|2|3, or with SSE=0 falling back to an "arch-native" approach). Limiting the static code path to SSE3 allows to practically target any deployed system, however using SSE=0 and AVX=0 together is falling back to generic code, and any static kernels are not specialized using the assembly code generator.</p>

<p>There are two main mechanisms to install LIBXSMM (both mechanisms can be combined): (1)\ building the library in an out-of-tree fashion, and (2)\ installing into a certain location. Building in an out-of-tree fashion looks like:</p>

<pre><code>cd libxsmm-install
make -f /path/to/libxsmm/Makefile
make clean
</code></pre>

<p>For example, installing into a specific location (incl. a selection of statically generated Intel SSE kernels) looks like:</p>

<pre><code>make MNK="1 2 3 4 5" PREFIX=/path/to/libxsmm-install install
make clean
</code></pre>

<p>Performing <code>make install-minimal</code> omits the documentation (default: <code>PREFIX/share/libxsmm</code>). Moreover, <code>PINCDIR</code>, <code>POUTDIR</code>, <code>PBINDIR</code>, and <code>PDOCDIR</code> allow to customize the locations underneath of the <code>PREFIX</code> location.</p>

<p><strong>NOTE</strong>: the library is agnostic with respect to the threading-runtime, and enabling OpenMP (OMP=1) when building the library is a non-default option (untested). The library is also agnostic with respect to the selected LAPACK/BLAS library if supported by the OS and link model (STATIC), and therefore linking GEMM routines when building the library (by supplying BLAS=1|2) may prevent a user from deciding at the time of linking the actual application.</p>

<h2>
<a id="running" class="anchor" href="#running" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running</h2>

<h3>
<a id="call-wrapper" class="anchor" href="#call-wrapper" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Call Wrapper</h3>

<p>Since the library is binary compatible with existing GEMM calls (LAPACK/BLAS), these calls can be replaced at link-time or intercepted at runtime of an application such that LIBXSMM is used instead of the original LAPACK/BLAS. Currently this only works for the Linux OS (not validated under OS X), and it is also not sufficient to rely on a GNU tool chain under Microsoft Windows. Of course, using LIBXSMM's programming interface when performing the same multiplication multiple time in a consecutive fashion (batch-processing) allows to extract higher performance. However, using the call wrapper might motivate to make use of the LIBXSMM API.</p>

<p>There are two cases to consider: (1)\ an application which is linking statically against LAPACK/BLAS, and (2)\ an application which is dynamically linked against LAPACK/BLAS. The first case requires making <code>-Wl,--wrap=xgemm_ /path/to/libxsmm.a</code> part of the link-line and then relinking the application:</p>

<pre><code>gcc [...] -Wl,--wrap=sgemm_ /path/to/libxsmm.a
gcc [...] -Wl,--wrap=dgemm_ /path/to/libxsmm.a
gcc [...] -Wl,--wrap=sgemm_,--wrap=dgemm_ /path/to/libxsmm.a
</code></pre>

<p>Relinking the application is often accomplished by copying, pasting, and modifying the linker command as shown when running "make" (or a similar build system), and then just re-invoking the modified link step. Please note that this first case is also working for an applications which is dynamically linked against LAPACK/BLAS.</p>

<p>If an application is dynamically linked against LAPACK/BLAS, the unmodified application allows for intercepting these calls at startup time (runtime) by using the LD_PRELOAD mechanism:</p>

<pre><code>LD_PRELOAD=/path/to/libxsmm.so ./myapplication
</code></pre>

<p>This case obviously requires to build a shared library of LIBXSMM:</p>

<pre><code>make STATIC=0
</code></pre>

<p>Please note that calling SGEMM is more sensitive to dispatch-overhead when compared to multiplying the same matrix sizes in double-precision. In case of single-precision, an approach of using the call wrapper is often not able to show an advantage if not regressing with respect to performance (therefore SGEMM is likely asking for making use of the API). In contrast, the double-precision case can show up to two times the performance of a typical LAPACK/BLAS performance (and more when using the API for processing batches).</p>

<h3>
<a id="call-trace" class="anchor" href="#call-trace" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Call Trace</h3>

<p>During the initial steps of employing the LIBXSMM API, one may rely on a debug version of the library (<code>make DBG=1</code>). The latter implies standard error (<code>stderr</code>) output in case of an error/warning condition inside of the library. Towards developing an application which is successfully using the library, being able to trace library calls might be useful as well (can be combined with DBG=1 or OPT=0):</p>

<pre><code>make TRACE=1
</code></pre>

<p>Building an application which is able to trace calls (inside of the library) requires the shared library of LIBXSMM, alternatively the application is required to link the static library in a dynamic fashion (GNU tool chain: -rdynamic). Actually tracing calls (without debugger) can be the accomplished by an environment variable called LIBXSMM_TRACE.</p>

<pre><code>LIBXSMM_TRACE=1 ./myapplication
</code></pre>

<p>Syntactically up to three arguments separated by commas (which allows to omit arguments) are taken (<code>tid,i,n</code>): <em>tid</em> signifies the ID of the thread to be traced with 1...NTHREADS being valid and where LIBXSMM_TRACE=1 is filtering for the "main thread" (in fact the first thread running into the trace facility); grabbing all threads (no filter) can be achieved by supplying a negative id (which is also the default when omitted). The second argument is pruning higher levels of the call-tree with <em>i=1</em> being the default (level zero is the highest at the same level as the main function). The last argument is taking the number of inclusive call levels with <em>n=-1</em> being the default (signifying no filter).</p>

<p>Please note that the trace facility is severely impacting the performance (even with LIBXSMM_TRACE=0), and this is not just because of console output but rather since inlining (internal) function  might be prevented along with additional call overhead on each function entry and exit. Therefore debug symbols can be also enabled separately (<code>make SYM=1</code>; implied by TRACE=1 or DBG=1) which might be useful when profiling an application. No facility of the library (other than DBG or TRACE/LIBXSMM_TRACE) is performing visible (console) or other non-private I/O (files).</p>

<h2>
<a id="performance" class="anchor" href="#performance" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Performance</h2>

<h3>
<a id="tuning" class="anchor" href="#tuning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Tuning</h3>

<p>Specifying a particular code path is not really necessary if the JIT backend is not disabled. However, disabling JIT compilation, statically generating a collection of kernels, and targeting a specific instruction set extension for the entire library looks like:</p>

<pre><code>make JIT=0 AVX=3 MNK="1 2 3 4 5"
</code></pre>

<p>The above example builds a library which cannot be deployed to anything else but the Intel Knights Landing processor family ("KNL") or future Intel Xeon processors supporting foundational Intel AVX-512 instructions (AVX-512F). The latter might be even more adjusted by supplying MIC=1 (along with AVX=3), however this does not matter since critical code is in inline assembly (and not affected). Similarly, SSE=0 (or JIT=0 without SSE or AVX build flag) employs an "arch-native" approach whereas AVX=1, AVX=2 (with FMA), and AVX=3 are specifically selecting the kind of Intel AVX code. Moreover, controlling the target flags manually or adjusting the code optimizations is also possible. The following example is GCC-specific and corresponds to OPT=3, AVX=3, and MIC=1:</p>

<pre><code>make OPT=3 TARGET="-mavx512f -mavx512cd -mavx512er -mavx512pf"
</code></pre>

<p>An extended interface can be generated which allows to perform software prefetches. Prefetching data might be helpful when processing batches of matrix multiplications where the next operands are farther away or otherwise unpredictable in their memory location. The prefetch strategy can be specified similar as shown in the section <a href="#generator-driver">Generator Driver</a> i.e., by either using the number of the shown enumeration, or by exactly using the name of the prefetch strategy. The only exception is PREFETCH=1 which is enabling a default strategy ("AL2_BL2viaC" rather than "nopf"). The following example is requesting the "AL2jpst" strategy:</p>

<pre><code>make PREFETCH=8
</code></pre>

<p>The prefetch interface is extending the signature of all kernels by three arguments (pa, pb, and pc). These additional arguments are specifying the locations of the operands of the next multiplication (the next a, b, and c matrices).</p>

<p>Further, the generated interface of the library also encodes the parameters the library was built for (static information). This helps optimizing client code related to the library's functionality. For example, the LIBXSMM_MAX_* and LIBXSMM_AVG_* information can be used with the LIBXSMM_PRAGMA_LOOP_COUNT macro in order to hint loop trip counts when handling matrices related to the problem domain of LIBXSMM.</p>

<h3>
<a id="auto-dispatch" class="anchor" href="#auto-dispatch" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Auto-dispatch</h3>

<p>The function 'libxsmm_?mmdispatch' helps amortizing the cost of the dispatch when multiple calls with the same M, N, and K are needed. The automatic code dispatch is orchestrating two levels:</p>

<ol>
<li>Specialized routine (implemented in assembly code),</li>
<li>LAPACK/BLAS library call (fallback).</li>
</ol>

<p>Both levels are accessible directly (see <a href="#interface">Interface</a> section) allowing to customize the code dispatch. The fallback level may be supplied by the Intel Math Kernel Library (Intel MKL)\ 11.2 DIRECT CALL feature. </p>

<p>Further, a preprocessor symbol denotes the largest problem size (<em>M</em> x <em>N</em> x <em>K</em>) that belongs to the first level, and therefore determines if a matrix multiplication falls back to calling into the LAPACK/BLAS library alongside of LIBXSMM. The problem size threshold can be configured by using for example:</p>

<pre><code>make THRESHOLD=$((60 * 60 * 60))
</code></pre>

<p>The maximum of the given threshold and the largest requested specialization refines the value of the threshold. Please note that explicitly JITting and executing a kernel is possible and independent of the threshold. If a problem size is below the threshold, dispatching the code requires to figure out whether a specialized routine exists or not.</p>

<p>In order to minimize the probability of key collisions (code cache), the preferred precision of the statically generated code can be selected:</p>

<pre><code>make PRECISION=2
</code></pre>

<p>The default preference is to generate and register both single and double-precision code, and therefore no space in the dispatch table is saved (PRECISION=0). Specifying PRECISION=1|2 is only generating and registering either single-precision or double-precision code.</p>

<h3>
<a id="jit-backend" class="anchor" href="#jit-backend" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>JIT Backend</h3>

<p>There might be situations in which it is up-front not clear which problem sizes will be needed when running an application. In order to leverage LIBXSMM's high-performance kernels, the library implements a JIT (Just-In-Time) code generation backend which generates the requested kernels on the fly (in-memory). This is accomplished by emitting the corresponding byte-code directly into an executable buffer. The actual JIT code is generated according to the CPUID flags, and therefore does not rely on the code path selected when building the library. In the current implementation, some limitations apply to the JIT backend specifically:</p>

<ol>
<li>In order to stay agnostic to any threading model used, Pthread mutexes are guarding the updates of the JITted code cache (link line with -lpthread is required); building with OMP=1 employs an OpenMP critical section as an alternative locking mechanism.</li>
<li>There is no support for the Intel SSE (Intel Xeon 5500/5600 series) and IMCI (Intel Xeon Phi coprocessor code-named Knights Corner) instruction set extensions. However, statically generated SSE-kernels can be leveraged without disabling support for JITting AVX kernels.</li>
<li>There is no support for the Windows calling convention (only kernels with PREFETCH=0 signature).</li>
</ol>

<p>The JIT backend can also be disabled at build time (<code>make JIT=0</code>) as well as at runtime (<code>LIBXSMM_JIT=0</code>). The latter is an environment variable which is also allowing to set a code path independent of the CPUID (LIBXSMM_JIT=0|1|snb|hsw|knl). Please note that LIBXSMM_JIT=1 is only supported for symmetry, and this environment setup cannot enable the JIT backend if it was disabled at build time (JIT=0).</p>

<p>One can use the aforementioned THRESHOLD parameter to control the matrix sizes for which the JIT compilation will be automatically performed. However, explicitly requested kernels (by calling libxsmm_?mmdispatch) are not subject to a problem size threshold. In any case, JIT code generation can be used for accompanying statically generated code.</p>

<p>Note: Modern Linux kernels are supporting transparent huge pages (THP). LIBXSMM is sanitizing this feature when setting the permissions for pages holding the executable code. However, we measured up to 30% slowdown when running JITted code in cases where THP decided to deliver a huge page. For systems with Linux kernel 2.6.38 (or later) THP will be automatically disabled for the mmap'ed regions (using madvise).</p>

<h3>
<a id="generator-driver" class="anchor" href="#generator-driver" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Generator Driver</h3>

<p>In rare situations it might be useful to directly incorporate generated C code (with inline assembly regions). This is accomplished by invoking a driver program (with certain command line arguments). The driver program is built as part of LIBXSMM's build process (when requesting static code generation), but also available via a separate build target:</p>

<pre><code>make generator
bin/libxsmm_generator
</code></pre>

<p>The code generator driver program accepts the following arguments:</p>

<ol>
<li>dense/dense_asm/sparse (dense creates C code, dense_asm creates ASM)</li>
<li>Filename of a file to append to</li>
<li>Routine name to be created</li>
<li>M parameter</li>
<li>N parameter</li>
<li>K parameter</li>
<li>LDA (0 when 1. is "sparse" indicates A is sparse)</li>
<li>LDB (0 when 1. is "sparse" indicates B is sparse)</li>
<li>LDC parameter</li>
<li>alpha (-1 or 1)</li>
<li>beta (0 or 1)</li>
<li>Alignment override for A (1 auto, 0 no alignment)</li>
<li>Alignment override for C (1 auto, 0 no alignment)</li>
<li>Architecture (noarch, wsm, snb, hsw, knc, knl)</li>
<li>Prefetch strategy, see below enumeration (dense/dense_asm only)</li>
<li>single precision (SP), or double precision (DP)</li>
<li>CSC file (just required when 1. is "sparse"). Matrix market format.</li>
</ol>

<p>The prefetch strategy can be:</p>

<ol>
<li>"nopf": no prefetching at all, just 3 inputs (*A, *B, *C)</li>
<li>"pfsigonly": just prefetching signature, 6 inputs (*A, *B, *C, *A’, *B’, *C’)</li>
<li>"BL2viaC": uses accesses to *C to prefetch *B’</li>
<li>"AL2": uses accesses to *A to prefetch *A’</li>
<li>"curAL2": prefetches current *A ahead in the kernel</li>
<li>"AL2_BL2viaC": combines AL2 and BL2viaC</li>
<li>"curAL2_BL2viaC": combines curAL2 and BL2viaC</li>
<li>"AL2jpst": aggressive *A’ prefetch of first rows without any structure</li>
<li>"AL2jpst_BL2viaC": combines AL2jpst and BL2viaC</li>
</ol>

<p>Here are some examples of invoking the driver program:</p>

<pre><code>bin/libxsmm_generator dense foo.c foo 16 16 16 32 32 32 1 1 1 1 hsw nopf DP
bin/libxsmm_generator dense_asm foo.c foo 16 16 16 32 32 32 1 1 1 1 knl AL2_BL2viaC DP
bin/libxsmm_generator sparse foo.c foo 16 16 16 32 0 32 1 1 1 1 hsw nopf DP bar.csc
</code></pre>

<p>Please note, there are additional examples given in samples/generator and samples/seissol.</p>

<h3>
<a id="results" class="anchor" href="#results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h3>

<p>The library does not claim to be "optimal" or "best-performing", and the presented results are modeling a certain application which might be not representative in general. Instead, information on how to reproduce the results is given underneath of the presented results (figure 1-3).</p>

<p><img src="https://raw.githubusercontent.com/hfp/libxsmm/master/samples/cp2k/results/Xeon_E5-2699v3/1-cp2k-specialized.png" alt='This plot shows the performance (based on LIBXSMM 1.0) for a dual-socket Intel Xeon E5-2699v3 ("Haswell") shows a "compact selection" (to make the plot visually more appealing) out of 386 specializations as useful for CP2K Open Source Molecular Dynamics [1]. The code has been generated and built by running "./[make.sh](https://github.com/hfp/libxsmm/blob/master/make.sh) -cp2k AVX=2 test -j". This and below plots were generated by running "cd samples/cp2k ; ./[cp2k-plot.sh](https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-plot.sh) specialized cp2k-specialized.png -1". Please note, that larger problem sizes (MNK) carry a higher arithmetic intensity which usually leads to higher performance (less bottlenecked by memory bandwidth).'></p>

<blockquote>
<p>This plot shows the performance (based on LIBXSMM 1.0) for a dual-socket Intel Xeon E5-2699v3 ("Haswell") shows a "compact selection" (to make the plot visually more appealing) out of 386 specializations as useful for CP2K Open Source Molecular Dynamics [1]. The code has been generated and built by running "./<a href="https://github.com/hfp/libxsmm/blob/master/make.sh">make.sh</a> -cp2k AVX=2 test -j". This and below plots were generated by running "cd samples/cp2k ; ./<a href="https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-plot.sh">cp2k-plot.sh</a> specialized cp2k-specialized.png -1". Please note, that larger problem sizes (MNK) carry a higher arithmetic intensity which usually leads to higher performance (less bottlenecked by memory bandwidth).</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/hfp/libxsmm/master/samples/cp2k/results/Xeon_E5-2699v3/2-cp2k-specialized.png" alt='This plot summarizes the performance (based on LIBXSMM 1.0) of the generated kernels by averaging the results over K (and therefore the bar on the right hand side may not show the same maximum when compared to other plots). The performance is well-tuned across the parameter space with no "cold islands", and the lower left "cold" corner is fairly limited. Please refer to the first figure on how to reproduce the results.'></p>

<blockquote>
<p>This plot summarizes the performance (based on LIBXSMM 1.0) of the generated kernels by averaging the results over K (and therefore the bar on the right hand side may not show the same maximum when compared to other plots). The performance is well-tuned across the parameter space with no "cold islands", and the lower left "cold" corner is fairly limited. Please refer to the first figure on how to reproduce the results.</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/hfp/libxsmm/master/samples/cp2k/results/Xeon_E5-2699v3/3-cp2k-specialized.png" alt='This plot shows the arithmetic average (non-sliding) of the performance (based on LIBXSMM 1.0) with respect to groups of problem sizes (MNK). The problem sizes are binned into three groups according to the shown intervals: "Small", "Medium", and "Larger" (notice that "larger" may still not be a large problem size). Please refer to the first figure on how to reproduce the results.'></p>

<blockquote>
<p>This plot shows the arithmetic average (non-sliding) of the performance (based on LIBXSMM 1.0) with respect to groups of problem sizes (MNK). The problem sizes are binned into three groups according to the shown intervals: "Small", "Medium", and "Larger" (notice that "larger" may still not be a large problem size). Please refer to the first figure on how to reproduce the results.</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/hfp/libxsmm/master/samples/cp2k/results/Xeon_E5-2699v3/4-cp2k-specialized.png" alt='In order to further summarize the previous plots, this graph shows the cumulative distribution function (CDF) of the performance (based on LIBXSMM 1.0) across all cases. Similar to the median value at 50%, one can read for example that 100% of the cases are yielding less or equal the largest discovered value. The value highlighted by the arrows is usually the median value, the [plot script](https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-perf.plt) however attempts to highlight a single "fair performance value" representing all cases by linearly fitting the CDF, projecting onto the x-axis, and taking the midpoint of the projection (usually at 50%). Please note, that this diagram shows a statistical distribution and does not allow to identify any particular kernel. Moreover at any point of the x-axis ("Probability"), the "Compute Performance" and the "Memory Bandwidth" graph do not necessarily belong to the same kernel! Please refer to the first figure on how to reproduce the results.'></p>

<blockquote>
<p>In order to further summarize the previous plots, this graph shows the cumulative distribution function (CDF) of the performance (based on LIBXSMM 1.0) across all cases. Similar to the median value at 50%, one can read for example that 100% of the cases are yielding less or equal the largest discovered value. The value highlighted by the arrows is usually the median value, the <a href="https://github.com/hfp/libxsmm/blob/master/samples/cp2k/cp2k-perf.plt">plot script</a> however attempts to highlight a single "fair performance value" representing all cases by linearly fitting the CDF, projecting onto the x-axis, and taking the midpoint of the projection (usually at 50%). Please note, that this diagram shows a statistical distribution and does not allow to identify any particular kernel. Moreover at any point of the x-axis ("Probability"), the "Compute Performance" and the "Memory Bandwidth" graph do not necessarily belong to the same kernel! Please refer to the first figure on how to reproduce the results.</p>
</blockquote>

<p>Please note that comparing performance results depends on whether or not streaming the operands of the matrix multiplication. For example, running a matrix multiplication code many time with all operands covered by the L1 cache may have an emphasis towards an implementation which actually performs worse for the real workload (if this real workload needs to stream some or all operands from the main memory).</p>

<h2>
<a id="applications" class="anchor" href="#applications" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Applications</h2>

<p><strong>[1] <a href="https://cp2k.org/">https://cp2k.org/</a></strong>: Open Source Molecular Dynamics with its DBCSR component processing batches of small matrix multiplications ("matrix stacks") out of a problem-specific distributed block-sparse matrix. Starting with <a href="https://www.cp2k.org/version_history">CP2K 3.0</a>, LIBXSMM can be used to substitute CP2K's "libsmm" library. Prior to CP2K 3.0, only the <a href="https://github.com/cp2k/cp2k/tree/intel">Intel branch of CP2K</a> was integrating LIBXSMM (see <a href="https://github.com/hfp/libxsmm/raw/master/documentation/cp2k.pdf">https://github.com/hfp/libxsmm/raw/master/documentation/cp2k.pdf</a>).</p>

<p><strong>[2] <a href="https://github.com/SeisSol/SeisSol/">https://github.com/SeisSol/SeisSol/</a></strong>: SeisSol is one of the leading codes for earthquake scenarios, in particular for simulating dynamic rupture processes. LIBXSMM provides highly optimized assembly kernels which form the computational back-bone of SeisSol (see <a href="https://github.com/TUM-I5/seissol_kernels/">https://github.com/TUM-I5/seissol_kernels/</a>).</p>

<h2>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h2>

<p><strong>[1] <a href="http://sc15.supercomputing.org/sites/all/themes/SC15images/tech_poster/tech_poster_pages/post137.html">http://sc15.supercomputing.org/sites/all/themes/SC15images/tech_poster/tech_poster_pages/post137.html</a></strong>: LIBXSMM: A High Performance Library for Small Matrix Multiplications (poster and two-page extended abstract). SC'15: The International Conference for High Performance Computing, Networking, Storage and Analysis, Austin (Texas).</p>

<p><strong>[2] <a href="http://software.intel.com/xeonphicatalog">http://software.intel.com/xeonphicatalog</a></strong>: Intel Xeon Phi Applications and Solutions Catalog.</p>

<p><strong>[3] <a href="https://software.intel.com/en-us/articles/intel-and-third-party-tools-and-libraries-available-with-support-for-intelr-xeon-phitm">http://goo.gl/qsnOOf</a></strong>: Intel 3rd Party Tools and Libraries.</p>
      </section>
      <footer>
        <p><small>Hosted on <a href="https://pages.github.com">GitHub Pages</a> using the Dinky theme</small></p>
      </footer>
    </div>
    <!--[if !IE]><script>fixScale(document);</script><![endif]-->
		          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-65670560-2");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
